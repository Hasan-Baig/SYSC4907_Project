{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hand_Gesture_Recognition_Model_Version_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "id": "l0kGrmGAf8s8",
        "outputId": "d62278b4-f99f-4c8d-d931-7e59066803f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-4996ee3d8d09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, use_metadata_server)\u001b[0m\n\u001b[1;32m    111\u001b[0m       \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m       \u001b[0muse_metadata_server\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_metadata_server\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m       ephemeral=ephemeral)\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, use_metadata_server, ephemeral)\u001b[0m\n\u001b[1;32m    134\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     _message.blocking_request(\n\u001b[0;32m--> 136\u001b[0;31m         'request_auth', request={'authType': 'dfs_ephemeral'}, timeout_sec=None)\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mmountpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    173\u001b[0m   request_id = send_request(\n\u001b[1;32m    174\u001b[0m       request_type, request, parent=parent, expect_reply=True)\n\u001b[0;32m--> 175\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    104\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    105\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "l20Rj1HP7LB-"
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# TensorFlow and tf.keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Helper libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Sklearn\n",
        "from sklearn.model_selection import train_test_split \n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sXBc72Ph89nR"
      },
      "cell_type": "markdown",
      "source": [
        "## Loading Data\n",
        "\n",
        "This project uses the [Hand Gesture Recognition Database](https://www.kaggle.com/gti-upm/leapgestrecog/version/1) available on Kaggle. It contains 20000 images with different hands and hand gestures. There is a total of 10 hand gestures of 10 different people presented in the dataset. (5 female and 5 male individuals). The images were captured using the Leap Motion hand tracking device.\n",
        "\n",
        ">Hand Gesture | Label \n",
        ">--- | ---\n",
        "> Thumb down | 0\n",
        "> Palm (Horizontal) | 1\n",
        "> L | 2\n",
        "> Fist (Horizontal) | 3\n",
        "> Fist (Vertical) | 4\n",
        "> Thumbs up | 5\n",
        "> Index | 6\n",
        "> OK | 7\n",
        "> Palm (Vertical) | 8\n",
        "> C | 9\n",
        "\n",
        ">Hand Gesture | Label \n",
        ">--- | ---\n",
        "> Palm | 0\n",
        "> L | 1\n",
        "> Fist | 2\n",
        "> One | 3\n",
        "> Five | 4\n",
        "> Straight | 5\n",
        "> Thumbs Up | 6\n",
        "> C | 7\n",
        "> Swing | 8\n",
        "\n",
        "Overview:\n",
        "- Load images\n",
        "- Validation\n",
        "- Preparing the images for training\n",
        "- Using train_test_split"
      ]
    },
    {
      "metadata": {
        "id": "pA0yTcvL_F_Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b7e355d-603a-4159-be8d-dd6167f313e8"
      },
      "cell_type": "code",
      "source": [
        "# Unzip images, ignore this cell if files are already in the workspace\n",
        "!unzip leapGestRecog.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open leapGestRecog.zip, leapGestRecog.zip.zip or leapGestRecog.zip.ZIP.\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "jRsNYIIoCmXp",
        "outputId": "d4ad4def-0bca-4cc1-af91-31fe8416ce89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "# We need to get all the paths for the images to later load them\n",
        "imagepaths = []\n",
        "\n",
        "# Go through all the files and subdirectories inside a folder and save path to images inside list\n",
        "for root, dirs, files in os.walk(\"/content/gdrive/MyDrive/PROJECT/leapGestRecog/\"): \n",
        "  for name in files:\n",
        "    path = os.path.join(root, name)\n",
        "    if path.endswith(\"png\"): # We want only the images\n",
        "      imagepaths.append(path)\n",
        "\n",
        "print(len(imagepaths)) # If > 0, then a PNG image was loaded"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "ps9mMyvx_MQR"
      },
      "cell_type": "code",
      "source": [
        "# This function is used more for debugging and showing results later. It plots the image into the notebook\n",
        "\n",
        "def plot_image(path):\n",
        "  img = cv2.imread(path) # Reads the image into a numpy.array\n",
        "  img_cvt = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) # Converts into the corret colorspace (RGB)\n",
        "  print(img_cvt.shape) # Prints the shape of the image just to check\n",
        "  plt.grid(False) # Without grid so we can see better\n",
        "  plt.imshow(img_cvt) # Shows the image\n",
        "  plt.xlabel(\"Width\")\n",
        "  plt.ylabel(\"Height\")\n",
        "  plt.title(\"Image \" + path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BJoQM_Yk__AX",
        "outputId": "073f71d8-da4b-4070-a378-a80ebd739506",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "cell_type": "code",
      "source": [
        "plot_image(imagepaths[0]) #We plot the first image from our imagepaths array"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-6440a778b85a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimagepaths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#We plot the first image from our imagepaths array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "mStukClHZm4p"
      },
      "cell_type": "markdown",
      "source": [
        "Imagepaths are now loaded so, next we have to prepare the images to train the algorithm. \n",
        "\n",
        "Load all the images into array **X** and all labels into array **y**."
      ]
    },
    {
      "metadata": {
        "id": "LuQLrqJ2ZmOM"
      },
      "cell_type": "code",
      "source": [
        "X = [] # Image data\n",
        "y = [] # Labels\n",
        "counter = 0\n",
        "\n",
        "# Loops through imagepaths to load images and labels into arrays\n",
        "for path in imagepaths:\n",
        "  print(counter)\n",
        "  img = cv2.imread(path) # Reads image and returns np.array\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) # Converts into the corret colorspace (GRAY)\n",
        "  img = cv2.resize(img, (320, 120)) # Reduce image size so traininreg can be faster\n",
        "  X.append(img)\n",
        "  \n",
        "  # Processing label in image path\n",
        "  category = path.split(\"/\")[7]\n",
        "  label = int(category.split(\"_\")[0][1]) # We need to convert 10_down to 00_down, or else it crashes\n",
        "  y.append(label)\n",
        "  \n",
        "  counter = counter + 1\n",
        "\n",
        "# Turn X and y into np.array to speed up train_test_split\n",
        "X = np.array(X, dtype=\"uint8\")\n",
        "X = X.reshape(len(imagepaths), 120, 320, 1) # Needed to reshape so CNN knows it's different images\n",
        "y = np.array(y)\n",
        "\n",
        "print(\"Images loaded: \", len(X))\n",
        "print(\"Labels loaded: \", len(y))\n",
        "\n",
        "print(y[0], imagepaths[0]) # Debugging"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8AjUu9S6d11-"
      },
      "cell_type": "markdown",
      "source": [
        "### Splitting data using train_test_split \n",
        "\n",
        "Data is split into a training set and a test set. The training set will be used to build our model. Then, the test data will be used to check if our predictions are correct. A random_state seed is used so the randomness of our results can be reproduced. The function will shuffle the images it's using to minimize training loss."
      ]
    },
    {
      "metadata": {
        "id": "NZa2YzKbdEz7"
      },
      "cell_type": "code",
      "source": [
        "ts = 0.3 # Percentage of images that we want to use for testing. The rest is used for training.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=ts, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "adafxZ8E8_ZB"
      },
      "cell_type": "markdown",
      "source": [
        "## Creating Model with CNN\n",
        "\n",
        "A Convolutional Neural Network (CNN) is a Deep Learning algorithm which can take in an input image, assign importance (learnable weights and biases) to various aspects/objects in the image and be able to differentiate one from the other. CNNs apply a series of filters to the raw pixel data of an image to extract and learn higher-level features, which the model can then use for classification. CNNs contains three components:\n",
        "\n",
        "- Convolutional layers, which apply a specified number of convolution filters to the image. For each subregion, the layer performs a set of mathematical operations to produce a single value in the output feature map. Convolutional layers then typically apply a ReLU activation function to the output to introduce nonlinearities into the model.\n",
        "\n",
        "- Pooling layers, which downsample the image data extracted by the convolutional layers to reduce the dimensionality of the feature map in order to decrease processing time. A commonly used pooling algorithm is max pooling, which extracts subregions of the feature map (e.g., 2x2-pixel tiles), keeps their maximum value, and discards all other values.\n",
        "\n",
        "- Dense (fully connected) layers, which perform classification on the features extracted by the convolutional layers and downsampled by the pooling layers. In a dense layer, every node in the layer is connected to every node in the preceding layer.\n",
        "\n",
        "Overview:\n",
        "- Import \n",
        "- Creation of CNN\n",
        "- Compiling and training model\n",
        "- Saving model for later use"
      ]
    },
    {
      "metadata": {
        "id": "Hq0ej5yDZq2e"
      },
      "cell_type": "code",
      "source": [
        "# Import of keras model and hidden layers for our convolutional network\n",
        "from keras.models import Sequential\n",
        "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
        "from keras.layers import Dense, Flatten"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iMiPmBWLYrkG"
      },
      "cell_type": "code",
      "source": [
        "# Construction of model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (5, 5), activation='relu', input_shape=(120, 320, 1))) \n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu')) \n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YliBFXpgLZic"
      },
      "cell_type": "markdown",
      "source": [
        "Conv2D layer parameters:\n",
        "- number of filters that the convolutional layer will learn. Layers early in the network architecture (closer to the actual input image) learn fewer convolutional filters while layers deeper in the network (closer to the output predictions) will learn more filters. This permits information to flow through the network without loss. These filters emulate edge detectors, blob detectors and other feature extraction methods.\n",
        "- kernel_size, a 2-tuple specifying the width and height of the 2D convolution window. The kernel_size must be an odd integer, with typical values of (1, 1) , (3, 3) , (5, 5) , (7, 7) . If the input images are greater than 128×128 it is recommended to test a kernel size > 3 to help learn larger spatial filters and to help reduce volume size.\n",
        "\n",
        "MaxPooling2D parameters\n",
        "- used to reduce the spatial dimensions of the output volume. It reduces processing time and allows assumptions to be made about features contained in the sub-regions. It is possible to notice in this network that our output spatial volume is decreasing and our number of filters learned is increasing. \n",
        "\n",
        "ReLU (rectified linear unit)\n",
        "- type of activation function. ReLU is the most commonly used activation function in neural networks, especially in CNNs. ReLU is linear (identity) for all positive values, and zero for all negative values. This means that it's cheap to compute as there is no complicated math. The model can therefore take less time to train or run. Also, it converges faster by applying non-linearities to the model, so there is no 'vanishing gradient problem' suffered by other activation functions like sigmoid or tanh."
      ]
    },
    {
      "metadata": {
        "id": "amOpISRiYw3z"
      },
      "cell_type": "code",
      "source": [
        "# Configures the model for training\n",
        "model.compile(optimizer='adam', # Optimization routine, which tells the computer how to adjust the parameter values to minimize the loss function.\n",
        "              loss='sparse_categorical_crossentropy', # Loss function, which tells us how bad our predictions are.\n",
        "              metrics=['accuracy']) # List of metrics to be evaluated by the model during training and testing."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adam \n",
        "- is an optimization algorithm that can be used instead of the classical stochastic gradient descent procedure to update network weights iterative based in training data. Combines:\n",
        "\n",
        "> - Adaptive Gradient Algorithm (AdaGrad) that maintains a per-parameter learning rate that improves performance on problems with sparse gradients (e.g. natural language and computer vision problems).\n",
        "> - Root Mean Square Propagation (RMSProp) that also maintains per-parameter learning rates that are adapted based on the average of recent magnitudes of the gradients for the weight (e.g. how quickly it is changing). This means the algorithm does well on online and non-stationary problems (e.g. noisy).\n",
        "\n",
        "sparse_categorical_crossentropy\n",
        "- Computes the crossentropy loss between the labels and predictions.\n",
        "- Use this crossentropy loss function when there are two or more label classes. We expect labels to be provided as integers."
      ],
      "metadata": {
        "id": "WvTg7mS0tB6z"
      }
    },
    {
      "metadata": {
        "id": "GMzJWqkGbkYl"
      },
      "cell_type": "code",
      "source": [
        "# Trains the model for a given number of epochs (iterations on a dataset) and validates it.\n",
        "model.fit(X_train, y_train, epochs=5, batch_size=64, verbose=2, validation_data=(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n1zE7Dn4fJt0"
      },
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JrWvHfD09BUj"
      },
      "cell_type": "markdown",
      "source": [
        "## Testing Model\n",
        "\n",
        "Now that we have the model compiled and trained, we need to check if it's good. First, we run ```model.evaluate``` to test the accuracy. Then, we make predictions and plot the images as long with the predicted labels and true labels to check everything. With that, we can see how our algorithm is working.  \n",
        "Later, we produce a confusion matrix, which is a specific table layout that allows visualization of the performance of an algorithm. \n",
        "\n",
        "Overview:\n",
        "- Evaluate model\n",
        "- Predictions\n",
        "- Plot images with predictions\n",
        "- Visualize model"
      ]
    },
    {
      "metadata": {
        "id": "TjFW-Rx1lGaF"
      },
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "\n",
        "print('Test accuracy: {:2.2f}%'.format(test_acc*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p8kgF7qFhahe"
      },
      "cell_type": "code",
      "source": [
        "predictions = model.predict(X_test) # Make predictions towards the test set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-LGVByVLlx0Q"
      },
      "cell_type": "code",
      "source": [
        "np.argmax(predictions[0]), y_test[0] # If same, got it right"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0xurvEqibvZf"
      },
      "cell_type": "code",
      "source": [
        "# Function to plot images and labels for validation purposes\n",
        "def validate_9_images(predictions_array, true_label_array, img_array):\n",
        "  # Array for pretty printing and then figure size\n",
        "  class_names = [\"down\", \"palm\", \"l\", \"fist\", \"fist_moved\", \"thumb\", \"index\", \"ok\", \"palm_moved\", \"c\"] \n",
        "  plt.figure(figsize=(15,5))\n",
        "  \n",
        "  for i in range(1, 10):\n",
        "    # Just assigning variables\n",
        "    prediction = predictions_array[i]\n",
        "    true_label = true_label_array[i]\n",
        "    img = img_array[i]\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
        "    \n",
        "    # Plot in a good way\n",
        "    plt.subplot(3,3,i)\n",
        "    plt.grid(False)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.imshow(img, cmap=plt.cm.binary)\n",
        "\n",
        "    predicted_label = np.argmax(prediction) # Get index of the predicted label from prediction\n",
        "    \n",
        "    # Change color of title based on good prediction or not\n",
        "    if predicted_label == true_label:\n",
        "      color = 'blue'\n",
        "    else:\n",
        "      color = 'red'\n",
        "\n",
        "    plt.xlabel(\"Predicted: {} {:2.0f}% (True: {})\".format(class_names[predicted_label],\n",
        "                                  100*np.max(prediction),\n",
        "                                  class_names[true_label]),\n",
        "                                  color=color)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QSl0leiufKqF"
      },
      "cell_type": "code",
      "source": [
        "validate_9_images(predictions, y_test, X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BJJlLR0hPYp1"
      },
      "cell_type": "code",
      "source": [
        "y_pred = np.argmax(predictions, axis=1) # Transform predictions into 1-D array with label number"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Tdza8t63PB7V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "1ef5bb06-8847-463c-887d-7178f96ce486"
      },
      "cell_type": "code",
      "source": [
        "# H = Horizontal\n",
        "# V = Vertical\n",
        "\n",
        "pd.DataFrame(confusion_matrix(y_test, y_pred), \n",
        "             columns=[\"Predicted Thumb Down\", \"Predicted Palm (H)\", \"Predicted L\", \"Predicted Fist (H)\", \"Predicted Fist (V)\", \"Predicted Thumbs up\", \"Predicted Index\", \"Predicted OK\", \"Predicted Palm (V)\", \"Predicted C\"],\n",
        "             index=[\"Actual Thumb Down\", \"Actual Palm (H)\", \"Actual L\", \"Actual Fist (H)\", \"Actual Fist (V)\", \"Actual Thumbs up\", \"Actual Index\", \"Actual OK\", \"Actual Palm (V)\", \"Actual C\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c6bdd24592d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# V = Vertical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m pd.DataFrame(confusion_matrix(y_test, y_pred), \n\u001b[0m\u001b[1;32m      5\u001b[0m              \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Predicted Thumb Down\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Predicted Palm (H)\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Predicted L\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Predicted Fist (H)\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Predicted Fist (V)\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Predicted Thumbs up\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Predicted Index\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Predicted OK\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Predicted Palm (V)\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Predicted C\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m              index=[\"Actual Thumb Down\", \"Actual Palm (H)\", \"Actual L\", \"Actual Fist (H)\", \"Actual Fist (V)\", \"Actual Thumbs up\", \"Actual Index\", \"Actual OK\", \"Actual Palm (V)\", \"Actual C\"])\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "sVd6-ROyUY6e"
      },
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "\n",
        "Based on the results presented in the previous section, we can conclude that our algorithm successfully classifies different hand gestures images with enough confidence (>99%) based on a Deep Learning model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save entire model to a HDF5 file\n",
        "model.save('/content/gdrive/MyDrive/PROJECT2/saved_model/my_model_version_2.h5') "
      ],
      "metadata": {
        "id": "OWOcWEfEf-du"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}